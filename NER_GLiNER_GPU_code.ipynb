{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrT6cNOw9Tm2r00PsOw/D+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SM-Learning/advanced-rag-techniques/blob/main/NER_GLiNER_GPU_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import GPUtil\n",
        "import teradatasql\n",
        "import inflect\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "from itertools import chain\n",
        "from queue import Queue\n",
        "from threading import Event, Thread\n"
      ],
      "metadata": {
        "id": "eHVO1UuTxrZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Initialize inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Teradata connection parameters\n",
        "TERADATA_CONFIG = {\n",
        "    'host': 'hostname',\n",
        "    'user': 'user_id',\n",
        "    'password': 'password',\n",
        "    'logmech': 'LDAP'\n",
        "}"
      ],
      "metadata": {
        "id": "eJwBYFkvxvik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Table names\n",
        "SOURCE_TABLE = \"source_table\"\n",
        "TARGET_TABLE = \"xyz\"\n",
        "ERROR_LOG_TABLE = \"error_log\"\n"
      ],
      "metadata": {
        "id": "FSgLqcE-x1Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPUMonitor:\n",
        "    @staticmethod\n",
        "    def get_gpu_memory_usage(gpu_id: int) -> tuple:\n",
        "        gpu = GPUtil.getGPUs()[gpu_id]\n",
        "        return gpu.memoryUsed, gpu.memoryTotal\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_optimal_batch_size(gpu_id: int, current_batch_size: int) -> int:\n",
        "        used, total = GPUMonitor.get_gpu_memory_usage(gpu_id)\n",
        "        memory_utilization = used / total\n",
        "\n",
        "        if memory_utilization > 0.85:\n",
        "            return max(32, current_batch_size // 2)\n",
        "        elif memory_utilization < 0.50:\n",
        "            return min(256, current_batch_size * 2)\n",
        "        return current_batch_size\n"
      ],
      "metadata": {
        "id": "ewZMGuCyx5nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchMetrics:\n",
        "    def __init__(self):\n",
        "        self.successful_records = 0\n",
        "        self.failed_records = 0\n",
        "        self.current_batch = 0\n",
        "        self.total_batches = 0\n",
        "\n",
        "    def log_batch_metrics(self, batch_id: int, success_count: int, fail_count: int):\n",
        "        logging.info(f\"\"\"\n",
        "            Batch {batch_id} Metrics:\n",
        "            - Successful records: {success_count}\n",
        "            - Failed records: {fail_count}\n",
        "            - GPU Memory Usage: {GPUMonitor.get_gpu_memory_usage(0)}\n",
        "        \"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pFolBLmxx8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, batch_size: int = 50000):\n",
        "        self.batch_size = batch_size\n",
        "        self.queue = Queue(maxsize=2)\n",
        "        self.stop_event = Event()\n",
        "\n",
        "    async def fetch_batch_from_db(self, offset: int) -> pd.DataFrame:\n",
        "        query = f\"\"\"\n",
        "            SELECT TOP {self.batch_size}\n",
        "                zi_c_company_id, zi_es_ecid, zi_c_description, zi_industry_primary,\n",
        "                industries, sub_industries, top3_industries\n",
        "            FROM {SOURCE_TABLE}\n",
        "            WHERE row_number > {offset}\n",
        "            QUALIFY ROW_NUMBER() OVER (ORDER BY zi_c_company_id) > {offset}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with teradatasql.connect(**TERADATA_CONFIG) as conn:\n",
        "                return pd.read_sql(query, conn)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fatal error in database fetch: {e}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "MJkxAETrx_Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAerByfbxlKr"
      },
      "outputs": [],
      "source": [
        "class NERProcessor:\n",
        "    def __init__(self, gpu_ids: List[int]):\n",
        "        self.gpu_ids = gpu_ids\n",
        "        self.models = {}\n",
        "        self.batch_size = 128  # Initial batch size\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        model_configs = [\n",
        "            (\"knowledgator/gliner-multitask-large-v0.5\", 0.54),\n",
        "            (\"EmergentMethods/gliner_large_news-v2.1\", 0.7)\n",
        "        ]\n",
        "\n",
        "        for gpu_id in self.gpu_ids:\n",
        "            self.models[gpu_id] = []\n",
        "            with torch.cuda.device(gpu_id):\n",
        "                for model_name, threshold in model_configs:\n",
        "                    model = GLiNER.from_pretrained(model_name).to(f\"cuda:{gpu_id}\")\n",
        "                    model.eval()\n",
        "                    self.models[gpu_id].append((model, threshold))\n",
        "\n",
        "    def clean_entity(self, entity: Dict) -> Dict:\n",
        "        try:\n",
        "            if entity['label'] == 'year started':\n",
        "                match = re.search(r'\\b(18|19|20)\\d{2}\\b', entity['text'])\n",
        "                entity['text'] = match.group(0) if match else ''\n",
        "            elif entity['label'] not in ['brand']:\n",
        "                entity['text'] = re.sub(r'[^a-zA-Z\\s]', '', entity['text'])\n",
        "                entity['text'] = p.singular_noun(entity['text']) or entity['text']\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in clean_entity: {e}\")\n",
        "        return entity\n",
        "\n",
        "    @torch.no_grad()\n",
        "    async def process_text(self, text: str, gpu_id: int) -> Dict:\n",
        "        if len(text) < 50:  # Check minimum text length\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            entities = []\n",
        "            for model, threshold in self.models[gpu_id]:\n",
        "                with torch.cuda.device(gpu_id):\n",
        "                    batch_entities = model.predict_entities(text, labels, threshold=threshold)\n",
        "                    entities.extend(batch_entities)\n",
        "\n",
        "            entities = sorted(\n",
        "                (self.clean_entity(entity) for entity in entities),\n",
        "                key=lambda k: (k['label'], -k['score'])\n",
        "            )\n",
        "\n",
        "            return self.extract_ner_features(entities)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing text on GPU {gpu_id}: {e}\")\n",
        "            # Retry on different GPU if available\n",
        "            other_gpu = [g for g in self.gpu_ids if g != gpu_id][0]\n",
        "            return await self.process_text(text, other_gpu)\n",
        "\n",
        "    def extract_ner_features(self, entities: List[Dict]) -> Dict:\n",
        "        information = {}\n",
        "        entity_sets = {}\n",
        "\n",
        "        for entity in entities:\n",
        "            label = entity[\"label\"]\n",
        "            text = entity[\"text\"]\n",
        "\n",
        "            # Process specific entity types\n",
        "            if label == \"brand\":\n",
        "                text = text.replace('brands', '').replace('brand', '').strip()\n",
        "            elif label == \"industry\":\n",
        "                text = text.lower().replace('industry', '').strip()\n",
        "            elif label == \"product\":\n",
        "                text = text.lower().replace('products', '').replace('product', '').strip()\n",
        "\n",
        "            if label not in entity_sets:\n",
        "                entity_sets[label] = set()\n",
        "            entity_sets[label].add(text)\n",
        "\n",
        "        # Build information dictionary\n",
        "        fields = [\n",
        "            \"year started\", \"product\", \"brand\", \"business_categories\",\n",
        "            \"business_sub_categories\", \"industry\", \"business_services\", \"offer\"\n",
        "        ]\n",
        "\n",
        "        for field in fields:\n",
        "            if field in entity_sets:\n",
        "                information[field] = \" | \".join(entity_sets[field])\n",
        "\n",
        "        return information\n",
        "\n",
        "class TeradataManager:\n",
        "    def __init__(self, config: Dict, batch_size: int = 5000):\n",
        "        self.config = config\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    async def upload_batch(self, df: pd.DataFrame) -> bool:\n",
        "        try:\n",
        "            with teradatasql.connect(**self.config) as conn:\n",
        "                cursor = conn.cursor()\n",
        "\n",
        "                data = [tuple(x) for x in df.values]\n",
        "\n",
        "                for i in range(0, len(data), self.batch_size):\n",
        "                    batch = data[i:i + self.batch_size]\n",
        "                    cursor.executemany(self.get_insert_sql(), batch)\n",
        "\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Upload error: {e}\")\n",
        "            await self.log_error(df.iloc[0]['zi_c_company_id'], str(e))\n",
        "            return False\n",
        "\n",
        "    def get_insert_sql(self) -> str:\n",
        "        return f\"\"\"\n",
        "        INSERT INTO {TARGET_TABLE} (\n",
        "            zi_c_company_id, zi_es_ecid, zi_c_description, zi_industry_primary,\n",
        "            industries, sub_industries, top3_industries, year_started, product,\n",
        "            brand, industry, business_categories, business_sub_categories,\n",
        "            business_services, offer\n",
        "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\"\n",
        "\n",
        "    async def log_error(self, company_id: str, error_msg: str):\n",
        "        # Error logging table insert logic (commented out as requested)\n",
        "        pass\n",
        "\n",
        "async def main():\n",
        "    metrics = BatchMetrics()\n",
        "    gpu_processor = NERProcessor(gpu_ids=[0, 1])\n",
        "    data_loader = DataLoader()\n",
        "    db_manager = TeradataManager(TERADATA_CONFIG)\n",
        "\n",
        "    offset = 0\n",
        "    total_processed = 0\n",
        "\n",
        "    try:\n",
        "        while total_processed < 5_000_000:  # 5 million records\n",
        "            batch_df = await data_loader.fetch_batch_from_db(offset)\n",
        "            if batch_df.empty:\n",
        "                break\n",
        "\n",
        "            # Process batch\n",
        "            for gpu_id in gpu_processor.gpu_ids:\n",
        "                gpu_processor.batch_size = GPUMonitor.calculate_optimal_batch_size(\n",
        "                    gpu_id, gpu_processor.batch_size\n",
        "                )\n",
        "\n",
        "            processed_records = []\n",
        "            for _, row in batch_df.iterrows():\n",
        "                if len(row['zi_c_description']) >= 50:\n",
        "                    result = await gpu_processor.process_text(\n",
        "                        row['zi_c_description'],\n",
        "                        gpu_id=total_processed % 2  # Alternate between GPUs\n",
        "                    )\n",
        "                    processed_records.append({**row.to_dict(), **result})\n",
        "\n",
        "            # Prepare for upload\n",
        "            output_df = pd.DataFrame(processed_records)\n",
        "            success = await db_manager.upload_batch(output_df)\n",
        "\n",
        "            # Update metrics\n",
        "            metrics.log_batch_metrics(\n",
        "                offset // data_loader.batch_size,\n",
        "                len(processed_records),\n",
        "                len(batch_df) - len(processed_records)\n",
        "            )\n",
        "\n",
        "            offset += data_loader.batch_size\n",
        "            total_processed += len(batch_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Fatal error: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        logging.info(f\"Total records processed: {total_processed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "YJQQBKzCxo7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuJvhRc2lJbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Update 3_8"
      ],
      "metadata": {
        "id": "8YY5W8Q9lKcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_partition_to_teradata(partition_iter: Iterator[tuple]) -> Iterator[dict]:\n",
        "    \"\"\"Process and upload a partition of data to Teradata.\"\"\"\n",
        "    from pyspark.taskcontext import TaskContext\n",
        "    import teradatasql  # Ensure teradatasql is imported here\n",
        "\n",
        "    partition_id = TaskContext.get().partitionId()\n",
        "    partition_rows_processed = 0\n",
        "    partition_rows_failed = 0\n",
        "\n",
        "    # Convert iterator to list for length calculation and multiple passes\n",
        "    rows = list(partition_iter)\n",
        "\n",
        "    if not rows:\n",
        "        logging.warning(f\"Partition {partition_id}: Empty partition\")\n",
        "        return iter([])\n",
        "\n",
        "    logging.info(f\"Partition {partition_id}: Starting upload of {len(rows)} rows\")\n",
        "\n",
        "    try:\n",
        "        # Establish connection to Teradata\n",
        "        with teradatasql.connect(\n",
        "            host=hostname,\n",
        "            user=user_id,\n",
        "            password=password,\n",
        "            logmech=logmech\n",
        "        ) as con:\n",
        "            cursor = con.cursor()\n",
        "\n",
        "            # Define the SQL insert statement\n",
        "            insert_sql = f\"\"\"\n",
        "            INSERT INTO {table_name} (\n",
        "                zi_c_company_id, zi_es_ecid, zi_c_description, zi_industry_primary,\n",
        "                industries, sub_industries, top3_industries, year_started, product,\n",
        "                brand, industry, business_categories, business_sub_categories,\n",
        "                business_services, offer\n",
        "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\"\n",
        "\n",
        "            # Process rows in batches\n",
        "            total_batches = (len(rows) - 1) // batch_size + 1\n",
        "\n",
        "            for batch_num, i in enumerate(range(0, len(rows), batch_size), 1):\n",
        "                batch = rows[i:i + batch_size]\n",
        "                try:\n",
        "                    # Convert batch elements to tuples before executing\n",
        "                    batch_tuples = [tuple(row) for row in batch]\n",
        "                    cursor.executemany(insert_sql, batch_tuples)\n",
        "                    partition_rows_processed += len(batch)\n",
        "                    logging.info(f\"Partition {partition_id}: Completed batch {batch_num}/{total_batches} \"\n",
        "                                 f\"({len(batch)} rows)\")\n",
        "                except Exception as e:\n",
        "                    partition_rows_failed += len(batch)\n",
        "                    logging.error(f\"Partition {partition_id}: Batch {batch_num} failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Partition {partition_id}: Connection error: {str(e)}\")\n",
        "        partition_rows_failed += len(rows)\n",
        "\n",
        "    # Return statistics for this partition\n",
        "    return iter([{\n",
        "        'partition_id': partition_id,\n",
        "        'rows_processed': partition_rows_processed,\n",
        "        'rows_failed': partition_rows_failed\n",
        "    }])"
      ],
      "metadata": {
        "id": "BjeVqWPZlJZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single sample record\n",
        "sample_data = [(\n",
        "    '12345', 'ECID456', 'Sample Corp', 'Technology',\n",
        "    'Tech,Software', 'Cloud,Data', 'Tech,AI,Cloud', '2020',\n",
        "    'Software', 'SampleBrand', 'Technology', 'Enterprise',\n",
        "    'B2B', 'Consulting', 'Cloud Services'\n",
        ")]\n",
        "\n",
        "# Define schema explicitly\n",
        "schema = StructType([\n",
        "    StructField(\"zi_c_company_id\", StringType(), True),\n",
        "    StructField(\"zi_es_ecid\", StringType(), True),\n",
        "    StructField(\"zi_c_description\", StringType(), True),\n",
        "    StructField(\"zi_industry_primary\", StringType(), True),\n",
        "    StructField(\"industries\", StringType(), True),\n",
        "    StructField(\"sub_industries\", StringType(), True),\n",
        "    StructField(\"top3_industries\", StringType(), True),\n",
        "    StructField(\"year_started\", StringType(), True),\n",
        "    StructField(\"product\", StringType(), True),\n",
        "    StructField(\"brand\", StringType(), True),\n",
        "    StructField(\"industry\", StringType(), True),\n",
        "    StructField(\"business_categories\", StringType(), True),\n",
        "    StructField(\"business_sub_categories\", StringType(), True),\n",
        "    StructField(\"business_services\", StringType(), True),\n",
        "    StructField(\"offer\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "sample_df = spark.createDataFrame(sample_data, schema=schema)\n",
        "\n",
        "# Convert to RDD\n",
        "sample_rdd = sample_df.rdd.map(tuple)\n",
        "\n",
        "# Test the upload function\n",
        "results = sample_rdd.mapPartitions(upload_partition_to_teradata).collect()\n",
        "\n",
        "# Print results\n",
        "print(\"Results:\", results)"
      ],
      "metadata": {
        "id": "YGi5VtbblJWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVz-GgwTlJTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VgeLzJeDlJRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}