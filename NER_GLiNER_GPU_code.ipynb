{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWeFJWMQ8EL5LbkgGhwt1y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import GPUtil\n",
        "import teradatasql\n",
        "import inflect\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "from itertools import chain\n",
        "from queue import Queue\n",
        "from threading import Event, Thread\n"
      ],
      "metadata": {
        "id": "eHVO1UuTxrZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Initialize inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# Teradata connection parameters\n",
        "TERADATA_CONFIG = {\n",
        "    'host': 'hostname',\n",
        "    'user': 'user_id',\n",
        "    'password': 'password',\n",
        "    'logmech': 'LDAP'\n",
        "}"
      ],
      "metadata": {
        "id": "eJwBYFkvxvik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Table names\n",
        "SOURCE_TABLE = \"source_table\"\n",
        "TARGET_TABLE = \"xyz\"\n",
        "ERROR_LOG_TABLE = \"error_log\"\n"
      ],
      "metadata": {
        "id": "FSgLqcE-x1Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPUMonitor:\n",
        "    @staticmethod\n",
        "    def get_gpu_memory_usage(gpu_id: int) -> tuple:\n",
        "        gpu = GPUtil.getGPUs()[gpu_id]\n",
        "        return gpu.memoryUsed, gpu.memoryTotal\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_optimal_batch_size(gpu_id: int, current_batch_size: int) -> int:\n",
        "        used, total = GPUMonitor.get_gpu_memory_usage(gpu_id)\n",
        "        memory_utilization = used / total\n",
        "\n",
        "        if memory_utilization > 0.85:\n",
        "            return max(32, current_batch_size // 2)\n",
        "        elif memory_utilization < 0.50:\n",
        "            return min(256, current_batch_size * 2)\n",
        "        return current_batch_size\n"
      ],
      "metadata": {
        "id": "ewZMGuCyx5nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchMetrics:\n",
        "    def __init__(self):\n",
        "        self.successful_records = 0\n",
        "        self.failed_records = 0\n",
        "        self.current_batch = 0\n",
        "        self.total_batches = 0\n",
        "\n",
        "    def log_batch_metrics(self, batch_id: int, success_count: int, fail_count: int):\n",
        "        logging.info(f\"\"\"\n",
        "            Batch {batch_id} Metrics:\n",
        "            - Successful records: {success_count}\n",
        "            - Failed records: {fail_count}\n",
        "            - GPU Memory Usage: {GPUMonitor.get_gpu_memory_usage(0)}\n",
        "        \"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pFolBLmxx8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, batch_size: int = 50000):\n",
        "        self.batch_size = batch_size\n",
        "        self.queue = Queue(maxsize=2)\n",
        "        self.stop_event = Event()\n",
        "\n",
        "    async def fetch_batch_from_db(self, offset: int) -> pd.DataFrame:\n",
        "        query = f\"\"\"\n",
        "            SELECT TOP {self.batch_size}\n",
        "                zi_c_company_id, zi_es_ecid, zi_c_description, zi_industry_primary,\n",
        "                industries, sub_industries, top3_industries\n",
        "            FROM {SOURCE_TABLE}\n",
        "            WHERE row_number > {offset}\n",
        "            QUALIFY ROW_NUMBER() OVER (ORDER BY zi_c_company_id) > {offset}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with teradatasql.connect(**TERADATA_CONFIG) as conn:\n",
        "                return pd.read_sql(query, conn)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fatal error in database fetch: {e}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "MJkxAETrx_Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAerByfbxlKr"
      },
      "outputs": [],
      "source": [
        "class NERProcessor:\n",
        "    def __init__(self, gpu_ids: List[int]):\n",
        "        self.gpu_ids = gpu_ids\n",
        "        self.models = {}\n",
        "        self.batch_size = 128  # Initial batch size\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        model_configs = [\n",
        "            (\"knowledgator/gliner-multitask-large-v0.5\", 0.54),\n",
        "            (\"EmergentMethods/gliner_large_news-v2.1\", 0.7)\n",
        "        ]\n",
        "\n",
        "        for gpu_id in self.gpu_ids:\n",
        "            self.models[gpu_id] = []\n",
        "            with torch.cuda.device(gpu_id):\n",
        "                for model_name, threshold in model_configs:\n",
        "                    model = GLiNER.from_pretrained(model_name).to(f\"cuda:{gpu_id}\")\n",
        "                    model.eval()\n",
        "                    self.models[gpu_id].append((model, threshold))\n",
        "\n",
        "    def clean_entity(self, entity: Dict) -> Dict:\n",
        "        try:\n",
        "            if entity['label'] == 'year started':\n",
        "                match = re.search(r'\\b(18|19|20)\\d{2}\\b', entity['text'])\n",
        "                entity['text'] = match.group(0) if match else ''\n",
        "            elif entity['label'] not in ['brand']:\n",
        "                entity['text'] = re.sub(r'[^a-zA-Z\\s]', '', entity['text'])\n",
        "                entity['text'] = p.singular_noun(entity['text']) or entity['text']\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in clean_entity: {e}\")\n",
        "        return entity\n",
        "\n",
        "    @torch.no_grad()\n",
        "    async def process_text(self, text: str, gpu_id: int) -> Dict:\n",
        "        if len(text) < 50:  # Check minimum text length\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            entities = []\n",
        "            for model, threshold in self.models[gpu_id]:\n",
        "                with torch.cuda.device(gpu_id):\n",
        "                    batch_entities = model.predict_entities(text, labels, threshold=threshold)\n",
        "                    entities.extend(batch_entities)\n",
        "\n",
        "            entities = sorted(\n",
        "                (self.clean_entity(entity) for entity in entities),\n",
        "                key=lambda k: (k['label'], -k['score'])\n",
        "            )\n",
        "\n",
        "            return self.extract_ner_features(entities)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing text on GPU {gpu_id}: {e}\")\n",
        "            # Retry on different GPU if available\n",
        "            other_gpu = [g for g in self.gpu_ids if g != gpu_id][0]\n",
        "            return await self.process_text(text, other_gpu)\n",
        "\n",
        "    def extract_ner_features(self, entities: List[Dict]) -> Dict:\n",
        "        information = {}\n",
        "        entity_sets = {}\n",
        "\n",
        "        for entity in entities:\n",
        "            label = entity[\"label\"]\n",
        "            text = entity[\"text\"]\n",
        "\n",
        "            # Process specific entity types\n",
        "            if label == \"brand\":\n",
        "                text = text.replace('brands', '').replace('brand', '').strip()\n",
        "            elif label == \"industry\":\n",
        "                text = text.lower().replace('industry', '').strip()\n",
        "            elif label == \"product\":\n",
        "                text = text.lower().replace('products', '').replace('product', '').strip()\n",
        "\n",
        "            if label not in entity_sets:\n",
        "                entity_sets[label] = set()\n",
        "            entity_sets[label].add(text)\n",
        "\n",
        "        # Build information dictionary\n",
        "        fields = [\n",
        "            \"year started\", \"product\", \"brand\", \"business_categories\",\n",
        "            \"business_sub_categories\", \"industry\", \"business_services\", \"offer\"\n",
        "        ]\n",
        "\n",
        "        for field in fields:\n",
        "            if field in entity_sets:\n",
        "                information[field] = \" | \".join(entity_sets[field])\n",
        "\n",
        "        return information\n",
        "\n",
        "class TeradataManager:\n",
        "    def __init__(self, config: Dict, batch_size: int = 5000):\n",
        "        self.config = config\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    async def upload_batch(self, df: pd.DataFrame) -> bool:\n",
        "        try:\n",
        "            with teradatasql.connect(**self.config) as conn:\n",
        "                cursor = conn.cursor()\n",
        "\n",
        "                data = [tuple(x) for x in df.values]\n",
        "\n",
        "                for i in range(0, len(data), self.batch_size):\n",
        "                    batch = data[i:i + self.batch_size]\n",
        "                    cursor.executemany(self.get_insert_sql(), batch)\n",
        "\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Upload error: {e}\")\n",
        "            await self.log_error(df.iloc[0]['zi_c_company_id'], str(e))\n",
        "            return False\n",
        "\n",
        "    def get_insert_sql(self) -> str:\n",
        "        return f\"\"\"\n",
        "        INSERT INTO {TARGET_TABLE} (\n",
        "            zi_c_company_id, zi_es_ecid, zi_c_description, zi_industry_primary,\n",
        "            industries, sub_industries, top3_industries, year_started, product,\n",
        "            brand, industry, business_categories, business_sub_categories,\n",
        "            business_services, offer\n",
        "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\"\n",
        "\n",
        "    async def log_error(self, company_id: str, error_msg: str):\n",
        "        # Error logging table insert logic (commented out as requested)\n",
        "        pass\n",
        "\n",
        "async def main():\n",
        "    metrics = BatchMetrics()\n",
        "    gpu_processor = NERProcessor(gpu_ids=[0, 1])\n",
        "    data_loader = DataLoader()\n",
        "    db_manager = TeradataManager(TERADATA_CONFIG)\n",
        "\n",
        "    offset = 0\n",
        "    total_processed = 0\n",
        "\n",
        "    try:\n",
        "        while total_processed < 5_000_000:  # 5 million records\n",
        "            batch_df = await data_loader.fetch_batch_from_db(offset)\n",
        "            if batch_df.empty:\n",
        "                break\n",
        "\n",
        "            # Process batch\n",
        "            for gpu_id in gpu_processor.gpu_ids:\n",
        "                gpu_processor.batch_size = GPUMonitor.calculate_optimal_batch_size(\n",
        "                    gpu_id, gpu_processor.batch_size\n",
        "                )\n",
        "\n",
        "            processed_records = []\n",
        "            for _, row in batch_df.iterrows():\n",
        "                if len(row['zi_c_description']) >= 50:\n",
        "                    result = await gpu_processor.process_text(\n",
        "                        row['zi_c_description'],\n",
        "                        gpu_id=total_processed % 2  # Alternate between GPUs\n",
        "                    )\n",
        "                    processed_records.append({**row.to_dict(), **result})\n",
        "\n",
        "            # Prepare for upload\n",
        "            output_df = pd.DataFrame(processed_records)\n",
        "            success = await db_manager.upload_batch(output_df)\n",
        "\n",
        "            # Update metrics\n",
        "            metrics.log_batch_metrics(\n",
        "                offset // data_loader.batch_size,\n",
        "                len(processed_records),\n",
        "                len(batch_df) - len(processed_records)\n",
        "            )\n",
        "\n",
        "            offset += data_loader.batch_size\n",
        "            total_processed += len(batch_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Fatal error: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        logging.info(f\"Total records processed: {total_processed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "YJQQBKzCxo7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuJvhRc2lJbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Update 3_8"
      ],
      "metadata": {
        "id": "8YY5W8Q9lKcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import time\n",
        "import zlib\n",
        "import pickle\n",
        "import socket\n",
        "import logging\n",
        "from itertools import chain\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
        "from pyspark.taskcontext import TaskContext\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filename='ner_processing.log',\n",
        "    filemode='w'\n",
        ")\n",
        "logger = logging.getLogger(\"NER_Processing\")\n",
        "\n",
        "# Import inflect for singular noun conversion\n",
        "try:\n",
        "    import inflect\n",
        "    p = inflect.engine()\n",
        "except ImportError:\n",
        "    logger.warning(\"Inflect package not found. Singular noun conversion will be skipped.\")\n",
        "    class DummyInflect:\n",
        "        def singular_noun(self, text):\n",
        "            return False\n",
        "    p = DummyInflect()\n",
        "\n",
        "# Initialize Spark session with optimized configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NER Processing\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"200\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Define schemas\n",
        "def get_ner_schema():\n",
        "    \"\"\"Define the schema for NER entities.\"\"\"\n",
        "    return ArrayType(\n",
        "        StructType([\n",
        "            StructField(\"text\", StringType(), True),\n",
        "            StructField(\"label\", StringType(), True),\n",
        "            StructField(\"score\", FloatType(), True),\n",
        "            StructField(\"start\", StringType(), True),\n",
        "            StructField(\"end\", StringType(), True)\n",
        "        ])\n",
        "    )\n",
        "\n",
        "def get_processed_schema(input_schema):\n",
        "    \"\"\"Add NER fields to the input schema.\"\"\"\n",
        "    fields = list(input_schema.fields)\n",
        "    if not any(field.name == \"entities\" for field in fields):\n",
        "        fields.append(StructField(\"entities\", get_ner_schema(), True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def clean_entity(entity):\n",
        "    \"\"\"Clean and normalize entity text based on entity label.\"\"\"\n",
        "    try:\n",
        "        if entity['label'] == 'year started':\n",
        "            match = re.search(r'\\b(18|19|20)\\d{2}\\b', entity['text'])\n",
        "            entity['text'] = match.group(0) if match else ''\n",
        "        elif entity['label'] not in ['brand']:\n",
        "            entity['text'] = re.sub(r'[^a-zA-Z\\s]', '', entity['text'])\n",
        "            entity['text'] = p.singular_noun(entity['text']) or entity['text']\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cleaning entity: {e}\")\n",
        "    return entity\n",
        "\n",
        "# Load your models\n",
        "logger.info(\"Loading NER models\")\n",
        "load_start = time.time()\n",
        "\n",
        "# Replace these with your actual model loading code\n",
        "def load_model1():\n",
        "    # Your model loading code here\n",
        "    pass\n",
        "\n",
        "def load_model2():\n",
        "    # Your model loading code here\n",
        "    pass\n",
        "\n",
        "model1 = load_model1()\n",
        "model2 = load_model2()\n",
        "\n",
        "# Define labels\n",
        "label_list = [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]  # Replace with your actual labels\n",
        "\n",
        "logger.info(f\"Models loaded in {time.time() - load_start:.2f} seconds\")\n",
        "\n",
        "# Compress models to reduce broadcast size\n",
        "logger.info(\"Compressing models for broadcast\")\n",
        "compress_start = time.time()\n",
        "\n",
        "compressed_model1 = zlib.compress(pickle.dumps(model1))\n",
        "compressed_model2 = zlib.compress(pickle.dumps(model2))\n",
        "\n",
        "# Log compression stats\n",
        "original_size1 = len(pickle.dumps(model1)) / (1024 * 1024)\n",
        "compressed_size1 = len(compressed_model1) / (1024 * 1024)\n",
        "original_size2 = len(pickle.dumps(model2)) / (1024 * 1024)\n",
        "compressed_size2 = len(compressed_model2) / (1024 * 1024)\n",
        "\n",
        "logger.info(f\"Model1: {original_size1:.2f}MB -> {compressed_size1:.2f}MB \"\n",
        "           f\"({compressed_size1/original_size1*100:.1f}%)\")\n",
        "logger.info(f\"Model2: {original_size2:.2f}MB -> {compressed_size2:.2f}MB \"\n",
        "           f\"({compressed_size2/original_size2*100:.1f}%)\")\n",
        "logger.info(f\"Models compressed in {time.time() - compress_start:.2f} seconds\")\n",
        "\n",
        "# Free up driver memory\n",
        "del model1\n",
        "del model2\n",
        "gc.collect()\n",
        "\n",
        "# Broadcast variables\n",
        "logger.info(\"Broadcasting models to executors\")\n",
        "broadcast_start = time.time()\n",
        "\n",
        "broadcast_model1 = sc.broadcast(compressed_model1)\n",
        "broadcast_model2 = sc.broadcast(compressed_model2)\n",
        "labels = sc.broadcast(label_list)\n",
        "\n",
        "logger.info(f\"Models broadcast in {time.time() - broadcast_start:.2f} seconds\")\n",
        "\n",
        "def process_batch_with_ner(batch, model1, model2, labels_list):\n",
        "    \"\"\"\n",
        "    Process a batch of records with NER models.\n",
        "\n",
        "    Args:\n",
        "        batch: List of Row objects to process\n",
        "        model1: First NER model\n",
        "        model2: Second NER model\n",
        "        labels_list: List of valid entity labels\n",
        "\n",
        "    Returns:\n",
        "        List of processed records with NER entities\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for row in batch:\n",
        "        try:\n",
        "            # Convert Row to dictionary for easier manipulation\n",
        "            record = row.asDict()\n",
        "\n",
        "            # Extract text from the record - adjust field names as needed\n",
        "            text = record.get('text', '')\n",
        "            if not text:\n",
        "                # Skip records with no text\n",
        "                results.append(row)\n",
        "                continue\n",
        "\n",
        "            # Process with model1\n",
        "            entities1 = model1.extract_entities(text)\n",
        "\n",
        "            # Process with model2\n",
        "            entities2 = model2.extract_entities(text)\n",
        "\n",
        "            # Clean and combine entities\n",
        "            entities = sorted(\n",
        "                (clean_entity(entity) for entity in chain(entities1, entities2)),\n",
        "                key=lambda k: (k['label'], -k['score'])\n",
        "            )\n",
        "\n",
        "            # Filter by labels if needed\n",
        "            if labels_list:\n",
        "                entities = [e for e in entities if e['label'] in labels_list]\n",
        "\n",
        "            # Add entities to the record\n",
        "            record['entities'] = entities\n",
        "\n",
        "            # Convert back to Row-like structure and append to results\n",
        "            from pyspark.sql import Row\n",
        "            results.append(Row(**record))\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing record: {str(e)}\")\n",
        "            # Keep the original record in case of error\n",
        "            results.append(row)\n",
        "\n",
        "    return results\n",
        "\n",
        "def extract_ner_entities(iterator):\n",
        "    \"\"\"\n",
        "    Process each partition with batching to improve efficiency.\n",
        "    Uses broadcast variables that are already in scope.\n",
        "    \"\"\"\n",
        "    # Get the partition ID and executor information for logging\n",
        "    context = TaskContext.get()\n",
        "    partition_id = context.partitionId()\n",
        "\n",
        "    # Get a unique identifier for the executor\n",
        "    hostname = socket.gethostname()\n",
        "    pid = str(os.getpid())\n",
        "    executor_id = f\"{hostname}_{pid}\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    logger.info(f\"Starting partition {partition_id} on executor {executor_id}\")\n",
        "\n",
        "    try:\n",
        "        # Log the decompression of broadcast variables - only once per partition\n",
        "        logger.info(f\"Partition {partition_id}: Starting model decompression\")\n",
        "        decompression_start = time.time()\n",
        "\n",
        "        # Decompress models - ONLY ONCE PER PARTITION\n",
        "        model1 = pickle.loads(zlib.decompress(broadcast_model1.value))\n",
        "        model2 = pickle.loads(zlib.decompress(broadcast_model2.value))\n",
        "        labels_list = labels.value\n",
        "\n",
        "        decompression_time = time.time() - decompression_start\n",
        "        logger.info(f\"Partition {partition_id}: Models decompressed in {decompression_time:.2f} seconds\")\n",
        "\n",
        "        # Initialize counters\n",
        "        record_count = 0\n",
        "        batch_count = 0\n",
        "        processing_start = time.time()\n",
        "\n",
        "        # Process in batches\n",
        "        batch_size = 100  # Adjust based on your needs\n",
        "        current_batch = []\n",
        "        results = []\n",
        "\n",
        "        for row in iterator:\n",
        "            record_count += 1\n",
        "            current_batch.append(row)\n",
        "\n",
        "            # Process when batch is full\n",
        "            if len(current_batch) >= batch_size:\n",
        "                batch_count += 1\n",
        "                batch_start = time.time()\n",
        "\n",
        "                try:\n",
        "                    processed_batch = process_batch_with_ner(current_batch, model1, model2, labels_list)\n",
        "                    results.extend(processed_batch)\n",
        "\n",
        "                    batch_time = time.time() - batch_start\n",
        "                    logger.info(f\"Partition {partition_id}: Processed batch {batch_count} ({len(current_batch)} records) in {batch_time:.2f} seconds\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing batch {batch_count} in partition {partition_id}: {str(e)}\")\n",
        "                    # Add original records to results in case of batch failure\n",
        "                    results.extend(current_batch)\n",
        "\n",
        "                # Clear the batch\n",
        "                current_batch = []\n",
        "\n",
        "        # Process any remaining records in the last batch\n",
        "        if current_batch:\n",
        "            batch_count += 1\n",
        "            batch_start = time.time()\n",
        "\n",
        "            try:\n",
        "                processed_batch = process_batch_with_ner(current_batch, model1, model2, labels_list)\n",
        "                results.extend(processed_batch)\n",
        "\n",
        "                batch_time = time.time() - batch_start\n",
        "                logger.info(f\"Partition {partition_id}: Processed final batch {batch_count} ({len(current_batch)} records) in {batch_time:.2f} seconds\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing final batch in partition {partition_id}: {str(e)}\")\n",
        "                # Add original records to results in case of batch failure\n",
        "                results.extend(current_batch)\n",
        "\n",
        "        # Log completion\n",
        "        total_time = time.time() - start_time\n",
        "        logger.info(f\"Finished partition {partition_id} on {executor_id}. \"\n",
        "                   f\"Processed {record_count} records in {total_time:.2f} seconds\")\n",
        "\n",
        "        return iter(results)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Major error in partition {partition_id} on {executor_id}: {str(e)}\")\n",
        "        import traceback\n",
        "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "        # Re-raise to ensure Spark knows about the failure\n",
        "        raise\n",
        "\n",
        "# Read input data\n",
        "input_path = \"gs://your-bucket/input_path\"  # Replace with your actual input path\n",
        "logger.info(f\"Reading data from {input_path}\")\n",
        "input_df = spark.read.parquet(input_path)\n",
        "\n",
        "# Log initial DataFrame information\n",
        "partition_count = input_df.rdd.getNumPartitions()\n",
        "record_count = input_df.count()\n",
        "logger.info(f\"Input DataFrame has {record_count} records in {partition_count} partitions\")\n",
        "\n",
        "# Calculate optimal partitions based on data size and cluster\n",
        "executor_instances = int(sc.getConf().get(\"spark.executor.instances\", \"1\"))\n",
        "executor_cores = int(sc.getConf().get(\"spark.executor.cores\", \"1\"))\n",
        "total_cores = executor_instances * executor_cores\n",
        "\n",
        "# Target 2-3 tasks per core for good parallelism\n",
        "optimal_partitions = total_cores * 3\n",
        "\n",
        "# Ensure we don't create too many partitions for small datasets\n",
        "if record_count < 10000:\n",
        "    optimal_partitions = min(optimal_partitions, record_count // 100)\n",
        "\n",
        "logger.info(f\"Calculated optimal partitions: {optimal_partitions}\")\n",
        "\n",
        "if partition_count != optimal_partitions:\n",
        "    logger.info(f\"Repartitioning from {partition_count} to {optimal_partitions} partitions\")\n",
        "    input_df = input_df.repartition(optimal_partitions)\n",
        "\n",
        "# Get the processed schema\n",
        "input_schema = input_df.schema\n",
        "processed_schema = get_processed_schema(input_schema)\n",
        "logger.info(f\"Using processed schema: {processed_schema}\")\n",
        "\n",
        "# Process the DataFrame\n",
        "logger.info(\"Processing DataFrame with NER extraction\")\n",
        "processed_rdd = input_df.rdd.mapPartitions(extract_ner_entities)\n",
        "\n",
        "# Convert back to DataFrame with the correct schema\n",
        "processed_df = spark.createDataFrame(processed_rdd, processed_schema)\n",
        "\n",
        "# Force an action to ensure processing happens\n",
        "processed_count = processed_df.count()\n",
        "logger.info(f\"Processed {processed_count} records\")\n",
        "\n",
        "# Write output to GCP bucket\n",
        "output_path = \"gs://your-bucket/output_path\"  # Replace with your actual output path\n",
        "logger.info(f\"Writing results to {output_path}\")\n",
        "\n",
        "# Check if Industry_primary column exists\n",
        "has_industry = \"Industry_primary\" in processed_df.columns\n",
        "\n",
        "if has_industry:\n",
        "    # Partition by Industry_primary (low cardinality field)\n",
        "    logger.info(\"Partitioning output by Industry_primary\")\n",
        "\n",
        "    # Write with partitioning by Industry_primary\n",
        "    processed_df.write \\\n",
        "        .partitionBy(\"Industry_primary\") \\\n",
        "        .option(\"compression\", \"snappy\") \\\n",
        "        .option(\"maxRecordsPerFile\", 50000) \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(output_path)\n",
        "else:\n",
        "    # If Industry_primary doesn't exist, coalesce to 20 files\n",
        "    logger.info(\"Industry_primary column not found, coalescing to 20 files\")\n",
        "    processed_df.coalesce(20).write \\\n",
        "        .option(\"compression\", \"snappy\") \\\n",
        "        .option(\"maxRecordsPerFile\", 50000) \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .parquet(output_path)\n",
        "\n",
        "logger.info(f\"Results written to {output_path}\")"
      ],
      "metadata": {
        "id": "BjeVqWPZlJZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YGi5VtbblJWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVz-GgwTlJTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VgeLzJeDlJRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}