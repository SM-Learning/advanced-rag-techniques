{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2KGr9kxegNM"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1. SETUP & DATA DOWNLOAD\n",
        "# =========================\n",
        "\n",
        "# Install required packages (if running in Colab, uncomment these lines)\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install pandas scikit-learn matplotlib tqdm nltk tokenizers\n",
        "# !pip install torchtext\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import random\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import gc\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI2zIRbvjjGS",
        "outputId": "0a5fa594-944c-4738-869a-40daf5eabc3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b5a163b46f0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "#torch.cuda.manual_seed(SEED)\n",
        "#torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nYb_gzai5Uk"
      },
      "outputs": [],
      "source": [
        "# Create an unverified SSL context\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Kk77rdhbesD6",
        "outputId": "2d2106ca-ae03-4f5a-b165-1925d61e4070"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nDATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz\"\\nDATA_FILE = \"All_Beauty_5.json.gz\"\\nif not os.path.exists(DATA_FILE):\\n    print(\"Downloading dataset...\")\\n    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\\n    print(\"Download complete.\")\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the \"All Beauty\" dataset if not already present\n",
        "'''\n",
        "DATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz\"\n",
        "DATA_FILE = \"All_Beauty_5.json.gz\"\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "    print(\"Download complete.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBwzjN9xh6i2"
      },
      "outputs": [],
      "source": [
        "# Download the \"Books\" dataset if not already present\n",
        "DATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
        "DATA_FILE = \"Books_5.json.gz\"\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "    print(\"Download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU2R1hN6exug",
        "outputId": "7306f115-5ea3-4b1a-8a52-ca84a38016b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample records with all fields:\n",
            "    overall  verified   reviewTime      reviewerID        asin  \\\n",
            "0      5.0     False  03 30, 2005  A1REUF3A1YCPHM  0001713353   \n",
            "1      5.0      True  06 20, 2016   AVP0HXC9FG790  0001713353   \n",
            "\n",
            "                       style     reviewerName  \\\n",
            "0  {'Format:': ' Hardcover'}      TW Ervin II   \n",
            "1                        NaN  Amazon Customer   \n",
            "\n",
            "                                          reviewText  \\\n",
            "0  The King, the Mice and the Cheese by Nancy Gur...   \n",
            "1                                 The kids loved it!   \n",
            "\n",
            "                                     summary  unixReviewTime  \n",
            "0  A story children will love and learn from      1112140800  \n",
            "1                                 Five Stars      1466380800  \n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 2. DATA LOADING & PREVIEW\n",
        "# =========================\n",
        "\n",
        "# Load a sample of the data to preview all fields\n",
        "def load_sample_records(filename, n=2):\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        records = [json.loads(line) for _, line in zip(range(n), f)]\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "sample_df = load_sample_records(DATA_FILE, n=2)\n",
        "print(\"Sample records with all fields:\\n\", sample_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp0Zm8J3fCVX",
        "outputId": "cc35f4b9-b484-446a-ecb5-5242d2d367f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200028it [00:04, 48502.03it/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution (note: natural, not balanced):\n",
            " overall\n",
            "5    127726\n",
            "4     39172\n",
            "3     17622\n",
            "2      8427\n",
            "1      7053\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 3. LOAD & BALANCE DATASET\n",
        "# =========================\n",
        "\n",
        "# Load 200K records and keep only reviewText and overall\n",
        "def load_balanced_subset(filename, n_total=200000):\n",
        "    # Read all records\n",
        "    records = []\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, total=n_total):\n",
        "            rec = json.loads(line)\n",
        "            if 'reviewText' in rec and 'overall' in rec:\n",
        "                records.append({'reviewText': rec['reviewText'], 'overall': int(float(rec['overall']))})\n",
        "            if len(records) >= n_total:\n",
        "                break\n",
        "    df = pd.DataFrame(records)\n",
        "    # Remove any rows with missing data or out-of-range ratings\n",
        "    df = df[df['overall'].isin([1,2,3,4,5])].dropna()\n",
        "    # Note: For large datasets, natural distribution is often better for generalization.\n",
        "    # If you want to balance, uncomment the next lines.\n",
        "    # min_count = df['overall'].value_counts().min()\n",
        "    # df = df.groupby('overall').sample(n=min_count, random_state=SEED)\n",
        "    print(\"Class distribution (note: natural, not balanced):\\n\", df['overall'].value_counts())\n",
        "    return df\n",
        "\n",
        "df = load_balanced_subset(DATA_FILE, n_total=200000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZQ-VDcXjQnK",
        "outputId": "3ae454b2-c7e2-4a8f-bc8e-1ba61305014c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtKq9bzZiOKl"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4. BPE TOKENIZATION\n",
        "# =========================\n",
        "\n",
        "# Train a Byte-Pair Encoding (BPE) tokenizer on the review texts\n",
        "bpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bpe_tokenizer.train_from_iterator(df['reviewText'], vocab_size=50000, min_frequency=2, show_progress=True)\n",
        "bpe_tokenizer.enable_truncation(max_length=200)\n",
        "\n",
        "# Save and reload tokenizer for reproducibility\n",
        "bpe_tokenizer.save_model(\".\", \"books_bpe\")\n",
        "bpe_tokenizer = ByteLevelBPETokenizer(\"books_bpe-vocab.json\", \"books_bpe-merges.txt\")\n",
        "bpe_tokenizer.enable_truncation(max_length=200)\n",
        "\n",
        "# Tokenize all reviews\n",
        "def encode_bpe(text):\n",
        "    return bpe_tokenizer.encode(text).ids\n",
        "\n",
        "df['bpe_ids'] = df['reviewText'].apply(encode_bpe)\n",
        "\n",
        "# Pad/truncate to MAX_LEN=200\n",
        "MAX_LEN = 200\n",
        "def pad_seq(seq, max_len=MAX_LEN):\n",
        "    if len(seq) < max_len:\n",
        "        return seq + [0] * (max_len - len(seq))\n",
        "    else:\n",
        "        return seq[:max_len]\n",
        "\n",
        "df['bpe_ids'] = df['bpe_ids'].apply(lambda x: pad_seq(x, MAX_LEN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "T25BEl-NfHij",
        "outputId": "289096b8-142e-4b03-eef4-cf4ae0401175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 162000\n",
            "Validation set size: 18000\n",
            "Test set size: 20000\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"class ReviewDataset(Dataset):\\n    def __init__(self, df):\\n        self.seqs = np.stack(df['bpe_ids'].values)\\n        self.labels = df['overall'].values - 1  # 0-based classes\\n\\n    def __len__(self):\\n        return len(self.seqs)\\n\\n    def __getitem__(self, idx):\\n        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\\n\\ndataset = ReviewDataset(df)\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =========================\n",
        "# 5. DATASET & DATALOADER\n",
        "# =========================\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.seqs = np.stack(df['bpe_ids'].values)\n",
        "        self.labels = df['overall'].values - 1  # 0-based classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Create dataset splits\n",
        "dataset = ReviewDataset(df)\n",
        "\n",
        "# Split into train/val/test\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(len(dataset)),\n",
        "    test_size=0.1,\n",
        "    stratify=df['overall'],\n",
        "    random_state=SEED\n",
        ")\n",
        "train_idx, val_idx = train_test_split(\n",
        "    train_idx,\n",
        "    test_size=0.1,\n",
        "    stratify=df.iloc[train_idx]['overall'],\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Create subset datasets\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "val_ds = Subset(dataset, val_idx)\n",
        "test_ds = Subset(dataset, test_idx)\n",
        "\n",
        "# Print split sizes\n",
        "print(f\"Training set size: {len(train_ds)}\")\n",
        "print(f\"Validation set size: {len(val_ds)}\")\n",
        "print(f\"Test set size: {len(test_ds)}\")\n",
        "\n",
        "'''class ReviewDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.seqs = np.stack(df['bpe_ids'].values)\n",
        "        self.labels = df['overall'].values - 1  # 0-based classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "dataset = ReviewDataset(df)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HehmQ3y2fKkr"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6. MODEL ARCHITECTURE\n",
        "# =========================\n",
        "\n",
        "class EnhancedSentimentModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50000,\n",
        "        embed_dim=128,\n",
        "        cnn_out=150,\n",
        "        lstm_hidden=126,  # Makes lstm_hidden*2 (252) divisible by num_heads (6)\n",
        "        num_heads=6,\n",
        "        num_classes=5,\n",
        "        dropout=0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Ensure hidden size is divisible by num_heads\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        assert (lstm_hidden * 2) % num_heads == 0, \"lstm_hidden * 2 must be divisible by num_heads\"\n",
        "\n",
        "        # Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # Multi-channel CNN\n",
        "        self.conv3 = nn.Conv1d(embed_dim, cnn_out, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(embed_dim, cnn_out, kernel_size=4, padding=2)\n",
        "        self.conv5 = nn.Conv1d(embed_dim, cnn_out, kernel_size=5, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_out)\n",
        "        self.bn4 = nn.BatchNorm1d(cnn_out)\n",
        "        self.bn5 = nn.BatchNorm1d(cnn_out)\n",
        "\n",
        "        # Residual projection\n",
        "        self.proj = nn.Linear(embed_dim, cnn_out * 3)\n",
        "\n",
        "        # BiLSTM\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=cnn_out * 3,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.ln_lstm = nn.LayerNorm(lstm_hidden * 2)\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.ln_attn = nn.LayerNorm(lstm_hidden * 2)\n",
        "\n",
        "        # Output\n",
        "        self.fc = nn.Linear(lstm_hidden * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        emb = self.embedding(x)  # (batch, seq, embed_dim)\n",
        "        emb_t = emb.transpose(1, 2)  # (batch, embed_dim, seq)\n",
        "\n",
        "        # Multi-channel CNN\n",
        "        c3 = F.relu(self.bn3(self.conv3(emb_t)))\n",
        "        c4 = F.relu(self.bn4(self.conv4(emb_t)))\n",
        "        c5 = F.relu(self.bn5(self.conv5(emb_t)))\n",
        "\n",
        "        # Adjust the size of c4 to match c3 and c5\n",
        "        c4 = c4[:, :, :-1]  # Remove the extra element from the sequence dimension\n",
        "\n",
        "        # Concatenate CNN outputs\n",
        "        cnn_cat = torch.cat([c3, c4, c5], dim=1)  # (batch, cnn_out*3, seq)\n",
        "        cnn_cat = cnn_cat.transpose(1, 2)  # (batch, seq, cnn_out*3)\n",
        "\n",
        "        # Residual connection\n",
        "        emb_proj = self.proj(emb)  # (batch, seq, cnn_out*3)\n",
        "        x_cnn = cnn_cat + emb_proj\n",
        "\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = self.bilstm(x_cnn)\n",
        "        lstm_out = self.ln_lstm(lstm_out)\n",
        "\n",
        "        # Multi-head attention\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        attn_out = self.ln_attn(lstm_out + attn_out)  # Residual connection\n",
        "\n",
        "        # Global average pooling\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "\n",
        "        # Output\n",
        "        out = self.dropout(pooled)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SckD5HPBfKna"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7. TRAINING & EVALUATION UTILS\n",
        "# =========================\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    patience=4,\n",
        "    max_lr=2e-3,\n",
        "    grad_clip=5.0,\n",
        "    device='cuda'\n",
        "):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=max_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 1cycle scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=max_lr,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for X, y in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Training Loss: {train_loss:.4f}')\n",
        "        print(f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print('Early stopping triggered')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def measure_inference_time(model, loader, device, n_batches=10):\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, (X, _) in enumerate(loader):\n",
        "            if i >= n_batches:\n",
        "                break\n",
        "            X = X.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(X)\n",
        "            times.append(time.time() - start)\n",
        "    avg_time = np.mean(times)\n",
        "    print(f\"Average inference time per batch: {avg_time:.4f} seconds\")\n",
        "    return avg_time\n",
        "\n",
        "def print_memory_footprint():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem = process.memory_info().rss / 1024**2\n",
        "    print(f\"CPU Memory usage: {mem:.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU Allocated:\", torch.cuda.memory_allocated()//1024**2, \"MB\")\n",
        "        print(\"GPU Cached:   \", torch.cuda.memory_reserved()//1024**2, \"MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfoH9G_FfKq-",
        "outputId": "7cca00ec-4668-48ac-f312-63eb530d2933"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EnhancedSentimentModel(\n",
            "  (embedding): Embedding(50000, 128, padding_idx=0)\n",
            "  (conv3): Conv1d(128, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv4): Conv1d(128, 150, kernel_size=(4,), stride=(1,), padding=(2,))\n",
            "  (conv5): Conv1d(128, 150, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (bn3): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (proj): Linear(in_features=128, out_features=450, bias=True)\n",
            "  (bilstm): LSTM(450, 126, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (ln_lstm): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=252, out_features=252, bias=True)\n",
            "  )\n",
            "  (ln_attn): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Linear(in_features=252, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n",
            "\n",
            "Model parameters: 7912761\n",
            "Using device: cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 1265/1265 [2:56:59<00:00,  8.39s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20:\n",
            "Training Loss: 0.9386\n",
            "Validation Loss: 0.8724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20:  43%|████▎     | 549/1265 [1:16:56<1:37:52,  8.20s/it]"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 8. TRAINING EXECUTION\n",
        "# =========================\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = EnhancedSentimentModel(\n",
        "    vocab_size=50000,\n",
        "    embed_dim=128,\n",
        "    cnn_out=150,\n",
        "    lstm_hidden=126,  # Makes lstm_hidden*2 (252) divisible by num_heads (6)\n",
        "    num_heads=6,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(\"\\nModel parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Train model\n",
        "trained_model, train_losses, val_losses = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    patience=4,\n",
        "    max_lr=2e-3,\n",
        "    grad_clip=5.0,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDS60n3QfVyk"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 9. EVALUATION & VISUALIZATION\n",
        "# =========================\n",
        "\n",
        "# Evaluate on test set\n",
        "preds, labels = evaluate_model(trained_model, test_loader, device)\n",
        "print(\"\\nTest Set Classification Report:\")\n",
        "print(classification_report(labels, preds, zero_division=0))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(labels, preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[1,2,3,4,5])\n",
        "disp.plot()\n",
        "plt.title(\"Test Set Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(\"Learning Curves\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Measure inference time and memory usage\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "measure_inference_time(trained_model, test_loader, device)\n",
        "print_memory_footprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyElJM2QfV27"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2 - Use all GPUs"
      ],
      "metadata": {
        "id": "_QvgF4PstUTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Y_TTtifV6g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,nounits,noheader'],\n",
        "        stdout=subprocess.PIPE, text=True\n",
        "    )\n",
        "    for idx, line in enumerate(result.stdout.strip().split('\\n')):\n",
        "        util, mem_used, mem_total = line.split(',')\n",
        "        print(f\"GPU {idx}: Utilization: {util.strip()}%, Memory: {mem_used.strip()} MiB / {mem_total.strip()} MiB\")\n",
        "\n",
        "def ddp_train(rank, world_size, model_class, dataset, batch_size, num_epochs, lr, grad_clip, results_dict):\n",
        "    # DDP setup\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "    torch.cuda.set_device(rank)\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "\n",
        "    # Distributed sampler\n",
        "    train_sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
        "\n",
        "    # Model, optimizer, loss\n",
        "    model = model_class().to(device)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    epoch_times = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        epoch_times.append(epoch_time)\n",
        "        avg_loss = running_loss / len(train_loader.dataset)\n",
        "        acc = correct / total\n",
        "\n",
        "        if rank == 0:\n",
        "            print(f\"[GPU {rank}] Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f} | Time: {epoch_time:.2f}s\")\n",
        "            print_gpu_utilization()\n",
        "\n",
        "    # Gather metrics from all ranks\n",
        "    if rank == 0:\n",
        "        results_dict['avg_epoch_time'] = sum(epoch_times) / len(epoch_times)\n",
        "        results_dict['final_acc'] = acc\n",
        "        results_dict['final_loss'] = avg_loss\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def run_ddp_experiment(model_class, dataset, batch_size=128, num_epochs=3, lr=2e-3, grad_clip=5.0, num_gpus=1):\n",
        "    manager = mp.Manager()\n",
        "    results_dict = manager.dict()\n",
        "    mp.spawn(\n",
        "        ddp_train,\n",
        "        args=(num_gpus, model_class, dataset, batch_size, num_epochs, lr, grad_clip, results_dict),\n",
        "        nprocs=num_gpus,\n",
        "        join=True\n",
        "    )\n",
        "    print(f\"\\n=== Results for {num_gpus} GPU(s) ===\")\n",
        "    for k, v in results_dict.items():\n",
        "        print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flexible GPU usage: Set num_gpus to 1, 2, or 4 to test scaling.\n",
        "Tracks:\n",
        "Per-epoch time\n",
        "Final accuracy and loss\n",
        "GPU utilization and memory (printed per epoch, per GPU)\n",
        "Creative metric tracking: Results are printed and can be logged for later analysis."
      ],
      "metadata": {
        "id": "Bq9SuVvPue_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEGcWL2GfdKp"
      },
      "outputs": [],
      "source": [
        "## Execute\n",
        "\n",
        "# Try with 1, 2, and 4 GPUs\n",
        "for NUM_GPUS in [1, 2, 4]:\n",
        "    run_ddp_experiment(\n",
        "        model_class=lambda: EnhancedSentimentModel(\n",
        "            vocab_size=50000,\n",
        "            embed_dim=128,\n",
        "            cnn_out=150,\n",
        "            lstm_hidden=192,   # Example: larger LSTM\n",
        "            num_heads=12,\n",
        "            num_classes=5,\n",
        "            dropout=0.3\n",
        "        ),\n",
        "        dataset=train_ds,\n",
        "        batch_size=128,\n",
        "        num_epochs=3,   # For sizing, 3 epochs is enough\n",
        "        lr=2e-3,\n",
        "        grad_clip=5.0,\n",
        "        num_gpus=NUM_GPUS\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "e4AmAj8CiLgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swJqgTohfWLK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbz6/yboMb/TjhYeYiQvnb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}