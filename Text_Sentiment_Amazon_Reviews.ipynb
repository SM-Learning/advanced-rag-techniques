{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh+JmVds6eOuqRJDf2yKJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SM-Learning/advanced-rag-techniques/blob/main/Text_Sentiment_Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2KGr9kxegNM",
        "outputId": "acf995b8-15d0-42d6-99d1-2eeebe173d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9c182a88b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# =========================\n",
        "# 1. SETUP & DATA DOWNLOAD\n",
        "# =========================\n",
        "\n",
        "# Install required packages (if running in Colab, uncomment these lines)\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install pandas scikit-learn matplotlib tqdm nltk\n",
        "# !pip install torchsummary\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import random\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gc\n",
        "import time\n",
        "import psutil\n",
        "import ssl\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "#torch.cuda.manual_seed(SEED)\n",
        "#torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an unverified SSL context\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "metadata": {
        "id": "3nYb_gzai5Uk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the \"All Beauty\" dataset if not already present\n",
        "'''\n",
        "DATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz\"\n",
        "DATA_FILE = \"All_Beauty_5.json.gz\"\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "    print(\"Download complete.\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk77rdhbesD6",
        "outputId": "2fd0cbb7-94d7-4a55-dbdc-5fe329bb8278"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the \"Books\" dataset if not already present\n",
        "DATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
        "DATA_FILE = \"Books_5.json.gz\"\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "    print(\"Download complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBwzjN9xh6i2",
        "outputId": "daaf7960-95a6-4e9c-d2c2-b54883c76d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2. DATA LOADING & PREVIEW\n",
        "# =========================\n",
        "\n",
        "# Load a sample of the data to preview all fields\n",
        "def load_sample_records(filename, n=2):\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        records = [json.loads(line) for _, line in zip(range(n), f)]\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "sample_df = load_sample_records(DATA_FILE, n=2)\n",
        "print(\"Sample records with all fields:\\n\", sample_df)"
      ],
      "metadata": {
        "id": "kU2R1hN6exug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3. LOAD & BALANCE DATASET\n",
        "# =========================\n",
        "\n",
        "# Load 200K records and keep only reviewText and overall\n",
        "def load_balanced_subset(filename, n_total=200000):\n",
        "    # Read all records\n",
        "    records = []\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, total=n_total):\n",
        "            rec = json.loads(line)\n",
        "            if 'reviewText' in rec and 'overall' in rec:\n",
        "                records.append({'reviewText': rec['reviewText'], 'overall': int(float(rec['overall']))})\n",
        "            if len(records) >= n_total:\n",
        "                break\n",
        "    df = pd.DataFrame(records)\n",
        "    # Remove any rows with missing data or out-of-range ratings\n",
        "    df = df[df['overall'].isin([1,2,3,4,5])].dropna()\n",
        "    # Note: For large datasets, natural distribution is often better for generalization.\n",
        "    # If you want to balance, uncomment the next lines.\n",
        "    # min_count = df['overall'].value_counts().min()\n",
        "    # df = df.groupby('overall').sample(n=min_count, random_state=SEED)\n",
        "    print(\"Class distribution (note: natural, not balanced):\\n\", df['overall'].value_counts())\n",
        "    return df\n",
        "\n",
        "df = load_balanced_subset(DATA_FILE, n_total=200000)"
      ],
      "metadata": {
        "id": "xp0Zm8J3fCVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4ZQ-VDcXjQnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4. BPE TOKENIZATION\n",
        "# =========================\n",
        "\n",
        "# Train a Byte-Pair Encoding (BPE) tokenizer on the review texts\n",
        "bpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bpe_tokenizer.train_from_iterator(df['reviewText'], vocab_size=50000, min_frequency=2, show_progress=True)\n",
        "bpe_tokenizer.enable_truncation(max_length=200)\n",
        "\n",
        "# Save and reload tokenizer for reproducibility\n",
        "bpe_tokenizer.save_model(\".\", \"books_bpe\")\n",
        "bpe_tokenizer = ByteLevelBPETokenizer(\"books_bpe-vocab.json\", \"books_bpe-merges.txt\")\n",
        "bpe_tokenizer.enable_truncation(max_length=200)\n",
        "\n",
        "# Tokenize all reviews\n",
        "def encode_bpe(text):\n",
        "    return bpe_tokenizer.encode(text).ids\n",
        "\n",
        "df['bpe_ids'] = df['reviewText'].apply(encode_bpe)\n",
        "\n",
        "# Pad/truncate to MAX_LEN=200\n",
        "MAX_LEN = 200\n",
        "def pad_seq(seq, max_len=MAX_LEN):\n",
        "    if len(seq) < max_len:\n",
        "        return seq + [0] * (max_len - len(seq))\n",
        "    else:\n",
        "        return seq[:max_len]\n",
        "\n",
        "df['bpe_ids'] = df['bpe_ids'].apply(lambda x: pad_seq(x, MAX_LEN))"
      ],
      "metadata": {
        "id": "AtKq9bzZiOKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5. DATASET & DATALOADER\n",
        "# =========================\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.seqs = np.stack(df['bpe_ids'].values)\n",
        "        self.labels = df['overall'].values - 1  # 0-based classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "dataset = ReviewDataset(df)"
      ],
      "metadata": {
        "id": "T25BEl-NfHij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 6. MODEL: MULTI-CHANNEL CNN + RESIDUAL + BiLSTM + MULTIHEAD ATTENTION\n",
        "# =========================\n",
        "\n",
        "# Helper: Multi-head self-attention\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        out = self.ln(x + attn_out)  # Residual + LayerNorm\n",
        "        return out\n",
        "\n",
        "def build_model(\n",
        "    vocab_size=50000,\n",
        "    embed_dim=128,\n",
        "    cnn_out=150,\n",
        "    lstm_hidden=128,\n",
        "    num_classes=5,\n",
        "    num_heads=6,\n",
        "    dropout=0.3\n",
        "):\n",
        "    # Embedding\n",
        "    embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "    # Multi-channel CNN frontend\n",
        "    conv3 = nn.Conv1d(embed_dim, cnn_out, kernel_size=3, padding=1)\n",
        "    conv4 = nn.Conv1d(embed_dim, cnn_out, kernel_size=4, padding=2)\n",
        "    conv5 = nn.Conv1d(embed_dim, cnn_out, kernel_size=5, padding=2)\n",
        "    bn3 = nn.BatchNorm1d(cnn_out)\n",
        "    bn4 = nn.BatchNorm1d(cnn_out)\n",
        "    bn5 = nn.BatchNorm1d(cnn_out)\n",
        "\n",
        "    # Residual projection for embedding\n",
        "    proj = nn.Linear(embed_dim, cnn_out * 3)\n",
        "\n",
        "    # BiLSTM (2 layers, hidden=128, dropout=0.3)\n",
        "    bilstm = nn.LSTM(\n",
        "        input_size=cnn_out * 3,\n",
        "        hidden_size=lstm_hidden,\n",
        "        num_layers=2,\n",
        "        dropout=dropout,\n",
        "        batch_first=True,\n",
        "        bidirectional=True\n",
        "    )\n",
        "    # LayerNorm between LSTM layers\n",
        "    ln_lstm = nn.LayerNorm(lstm_hidden * 2)\n",
        "\n",
        "    # Multi-head self-attention\n",
        "    attn = MultiHeadSelfAttention(lstm_hidden * 2, num_heads)\n",
        "\n",
        "    # Output\n",
        "    fc = nn.Linear(lstm_hidden * 2, num_classes)\n",
        "    drop = nn.Dropout(dropout)\n",
        "\n",
        "    # Compose model as a function (not a class)\n",
        "    def model(x):\n",
        "        # x: (batch, seq)\n",
        "        emb = embedding(x)  # (batch, seq, embed_dim)\n",
        "        emb_t = emb.transpose(1, 2)  # (batch, embed_dim, seq)\n",
        "        # Multi-channel CNN\n",
        "        c3 = F.relu(bn3(conv3(emb_t)))\n",
        "        c4 = F.relu(bn4(conv4(emb_t)))\n",
        "        c5 = F.relu(bn5(conv5(emb_t)))\n",
        "        # (batch, cnn_out, seq)\n",
        "        cnn_cat = torch.cat([c3, c4, c5], dim=1)  # (batch, cnn_out*3, seq)\n",
        "        cnn_cat = cnn_cat.transpose(1, 2)  # (batch, seq, cnn_out*3)\n",
        "        # Residual connection from embedding\n",
        "        emb_proj = proj(emb)  # (batch, seq, cnn_out*3)\n",
        "        x_cnn = cnn_cat + emb_proj  # Residual add\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = bilstm(x_cnn)\n",
        "        lstm_out = ln_lstm(lstm_out)\n",
        "        # Multi-head self-attention\n",
        "        attn_out = attn(lstm_out)\n",
        "        # Pooling (mean)\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "        out = drop(pooled)\n",
        "        out = fc(out)\n",
        "        return out\n",
        "    return model"
      ],
      "metadata": {
        "id": "HehmQ3y2fKkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 7. TRAINING & EVALUATION UTILS\n",
        "# =========================\n",
        "\n",
        "def train_model(\n",
        "    model_fn,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    patience=4,\n",
        "    max_lr=2e-3,\n",
        "    grad_clip=5.0,\n",
        "    scheduler_type='1cycle'\n",
        "):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Model as a function, so wrap in nn.Module for optimizer\n",
        "    class Wrapper(nn.Module):\n",
        "        def __init__(self, model_fn):\n",
        "            super().__init__()\n",
        "            self.model_fn = model_fn\n",
        "        def forward(self, x):\n",
        "            return self.model_fn(x)\n",
        "    model = Wrapper(model_fn).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=max_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if scheduler_type == '1cycle':\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer, max_lr=max_lr, steps_per_epoch=len(train_loader), epochs=num_epochs\n",
        "        )\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
        "        )\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(X)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            # Gradient clipping (norm)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            if scheduler_type == '1cycle':\n",
        "                scheduler.step()\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                out = model(X)\n",
        "                loss = criterion(out, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "        if scheduler_type == 'plateau':\n",
        "            scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            out = model(X)\n",
        "            preds = torch.argmax(out, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def measure_inference_time(model, loader, n_batches=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, (X, _) in enumerate(loader):\n",
        "            if i >= n_batches:\n",
        "                break\n",
        "            X = X.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(X)\n",
        "            times.append(time.time() - start)\n",
        "    avg_time = np.mean(times)\n",
        "    print(f\"Average inference time per batch: {avg_time:.4f} seconds\")\n",
        "    return avg_time\n",
        "\n",
        "def print_memory_footprint():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem = process.memory_info().rss / 1024**2\n",
        "    print(f\"CPU Memory usage: {mem:.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU Allocated:\", torch.cuda.memory_allocated()//1024**2, \"MB\")\n",
        "        print(\"GPU Cached:   \", torch.cuda.memory_reserved()//1024**2, \"MB\")"
      ],
      "metadata": {
        "id": "SckD5HPBfKna"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8. DATA SPLIT & TRAINING\n",
        "# =========================\n",
        "\n",
        "# Split into train/val/test\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(len(df)), test_size=0.1, stratify=df['overall'], random_state=SEED\n",
        ")\n",
        "val_idx, train_idx = train_test_split(\n",
        "    train_idx, test_size=0.9, stratify=df.iloc[train_idx]['overall'], random_state=SEED\n",
        ")\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "val_ds = Subset(dataset, val_idx)\n",
        "test_ds = Subset(dataset, test_idx)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Build model\n",
        "model_fn = build_model(\n",
        "    vocab_size=50000,\n",
        "    embed_dim=128,\n",
        "    cnn_out=150,\n",
        "    lstm_hidden=128,\n",
        "    num_classes=5,\n",
        "    num_heads=6,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "# Train\n",
        "trained_model, train_losses, val_losses = train_model(\n",
        "    model_fn, train_loader, val_loader,\n",
        "    num_epochs=20, patience=4, max_lr=2e-3, grad_clip=5.0, scheduler_type='1cycle'\n",
        ")"
      ],
      "metadata": {
        "id": "rfoH9G_FfKq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 9. EVALUATION & VISUALIZATION\n",
        "# =========================\n",
        "\n",
        "# Evaluate on test set\n",
        "preds, labels = evaluate_model(trained_model, test_loader)\n",
        "print(classification_report(labels, preds, zero_division=0))\n",
        "cm = confusion_matrix(labels, preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[1,2,3,4,5])\n",
        "disp.plot()\n",
        "plt.title(\"Test Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.title(\"Learning Curves\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Inference time and memory\n",
        "measure_inference_time(trained_model, test_loader)\n",
        "print_memory_footprint()"
      ],
      "metadata": {
        "id": "jDS60n3QfVyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyElJM2QfV27"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8Y_TTtifV6g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEGcWL2GfdKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swJqgTohfWLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}