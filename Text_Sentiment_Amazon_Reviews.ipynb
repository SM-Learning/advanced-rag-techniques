{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkDYmRUS/AQNV08lufYEQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SM-Learning/advanced-rag-techniques/blob/main/Text_Sentiment_Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "b2KGr9kxegNM"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1. SETUP & DATA DOWNLOAD\n",
        "# =========================\n",
        "\n",
        "# Install required packages (if running in Colab, uncomment these lines)\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install pandas scikit-learn matplotlib tqdm nltk tokenizers\n",
        "# !pip install torchtext\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import random\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import gc\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "#torch.cuda.manual_seed(SEED)\n",
        "#torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI2zIRbvjjGS",
        "outputId": "0a5fa594-944c-4738-869a-40daf5eabc3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b5a163b46f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an unverified SSL context\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "metadata": {
        "id": "3nYb_gzai5Uk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the \"All Beauty\" dataset if not already present\n",
        "'''\n",
        "DATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz\"\n",
        "DATA_FILE = \"All_Beauty_5.json.gz\"\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "    print(\"Download complete.\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Kk77rdhbesD6",
        "outputId": "2d2106ca-ae03-4f5a-b165-1925d61e4070"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/All_Beauty_5.json.gz\"\\nDATA_FILE = \"All_Beauty_5.json.gz\"\\nif not os.path.exists(DATA_FILE):\\n    print(\"Downloading dataset...\")\\n    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\\n    print(\"Download complete.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the \"Books\" dataset if not already present\n",
        "DATA_URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\"\n",
        "DATA_FILE = \"Books_5.json.gz\"\n",
        "if not os.path.exists(DATA_FILE):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_FILE)\n",
        "    print(\"Download complete.\")"
      ],
      "metadata": {
        "id": "NBwzjN9xh6i2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2. DATA LOADING & PREVIEW\n",
        "# =========================\n",
        "\n",
        "# Load a sample of the data to preview all fields\n",
        "def load_sample_records(filename, n=2):\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        records = [json.loads(line) for _, line in zip(range(n), f)]\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "sample_df = load_sample_records(DATA_FILE, n=2)\n",
        "print(\"Sample records with all fields:\\n\", sample_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU2R1hN6exug",
        "outputId": "7306f115-5ea3-4b1a-8a52-ca84a38016b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample records with all fields:\n",
            "    overall  verified   reviewTime      reviewerID        asin  \\\n",
            "0      5.0     False  03 30, 2005  A1REUF3A1YCPHM  0001713353   \n",
            "1      5.0      True  06 20, 2016   AVP0HXC9FG790  0001713353   \n",
            "\n",
            "                       style     reviewerName  \\\n",
            "0  {'Format:': ' Hardcover'}      TW Ervin II   \n",
            "1                        NaN  Amazon Customer   \n",
            "\n",
            "                                          reviewText  \\\n",
            "0  The King, the Mice and the Cheese by Nancy Gur...   \n",
            "1                                 The kids loved it!   \n",
            "\n",
            "                                     summary  unixReviewTime  \n",
            "0  A story children will love and learn from      1112140800  \n",
            "1                                 Five Stars      1466380800  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3. LOAD & BALANCE DATASET\n",
        "# =========================\n",
        "\n",
        "# Load 200K records and keep only reviewText and overall\n",
        "def load_balanced_subset(filename, n_total=200000):\n",
        "    # Read all records\n",
        "    records = []\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, total=n_total):\n",
        "            rec = json.loads(line)\n",
        "            if 'reviewText' in rec and 'overall' in rec:\n",
        "                records.append({'reviewText': rec['reviewText'], 'overall': int(float(rec['overall']))})\n",
        "            if len(records) >= n_total:\n",
        "                break\n",
        "    df = pd.DataFrame(records)\n",
        "    # Remove any rows with missing data or out-of-range ratings\n",
        "    df = df[df['overall'].isin([1,2,3,4,5])].dropna()\n",
        "    # Note: For large datasets, natural distribution is often better for generalization.\n",
        "    # If you want to balance, uncomment the next lines.\n",
        "    # min_count = df['overall'].value_counts().min()\n",
        "    # df = df.groupby('overall').sample(n=min_count, random_state=SEED)\n",
        "    print(\"Class distribution (note: natural, not balanced):\\n\", df['overall'].value_counts())\n",
        "    return df\n",
        "\n",
        "df = load_balanced_subset(DATA_FILE, n_total=200000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp0Zm8J3fCVX",
        "outputId": "cc35f4b9-b484-446a-ecb5-5242d2d367f5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "200028it [00:04, 48502.03it/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution (note: natural, not balanced):\n",
            " overall\n",
            "5    127726\n",
            "4     39172\n",
            "3     17622\n",
            "2      8427\n",
            "1      7053\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZQ-VDcXjQnK",
        "outputId": "3ae454b2-c7e2-4a8f-bc8e-1ba61305014c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4. BPE TOKENIZATION\n",
        "# =========================\n",
        "\n",
        "# Train a Byte-Pair Encoding (BPE) tokenizer on the review texts\n",
        "bpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bpe_tokenizer.train_from_iterator(df['reviewText'], vocab_size=50000, min_frequency=2, show_progress=True)\n",
        "bpe_tokenizer.enable_truncation(max_length=200)\n",
        "\n",
        "# Save and reload tokenizer for reproducibility\n",
        "bpe_tokenizer.save_model(\".\", \"books_bpe\")\n",
        "bpe_tokenizer = ByteLevelBPETokenizer(\"books_bpe-vocab.json\", \"books_bpe-merges.txt\")\n",
        "bpe_tokenizer.enable_truncation(max_length=200)\n",
        "\n",
        "# Tokenize all reviews\n",
        "def encode_bpe(text):\n",
        "    return bpe_tokenizer.encode(text).ids\n",
        "\n",
        "df['bpe_ids'] = df['reviewText'].apply(encode_bpe)\n",
        "\n",
        "# Pad/truncate to MAX_LEN=200\n",
        "MAX_LEN = 200\n",
        "def pad_seq(seq, max_len=MAX_LEN):\n",
        "    if len(seq) < max_len:\n",
        "        return seq + [0] * (max_len - len(seq))\n",
        "    else:\n",
        "        return seq[:max_len]\n",
        "\n",
        "df['bpe_ids'] = df['bpe_ids'].apply(lambda x: pad_seq(x, MAX_LEN))"
      ],
      "metadata": {
        "id": "AtKq9bzZiOKl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5. DATASET & DATALOADER\n",
        "# =========================\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.seqs = np.stack(df['bpe_ids'].values)\n",
        "        self.labels = df['overall'].values - 1  # 0-based classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Create dataset splits\n",
        "dataset = ReviewDataset(df)\n",
        "\n",
        "# Split into train/val/test\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(len(dataset)),\n",
        "    test_size=0.1,\n",
        "    stratify=df['overall'],\n",
        "    random_state=SEED\n",
        ")\n",
        "train_idx, val_idx = train_test_split(\n",
        "    train_idx,\n",
        "    test_size=0.1,\n",
        "    stratify=df.iloc[train_idx]['overall'],\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# Create subset datasets\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "val_ds = Subset(dataset, val_idx)\n",
        "test_ds = Subset(dataset, test_idx)\n",
        "\n",
        "# Print split sizes\n",
        "print(f\"Training set size: {len(train_ds)}\")\n",
        "print(f\"Validation set size: {len(val_ds)}\")\n",
        "print(f\"Test set size: {len(test_ds)}\")\n",
        "\n",
        "'''class ReviewDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.seqs = np.stack(df['bpe_ids'].values)\n",
        "        self.labels = df['overall'].values - 1  # 0-based classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "dataset = ReviewDataset(df)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "T25BEl-NfHij",
        "outputId": "289096b8-142e-4b03-eef4-cf4ae0401175"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 162000\n",
            "Validation set size: 18000\n",
            "Test set size: 20000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class ReviewDataset(Dataset):\\n    def __init__(self, df):\\n        self.seqs = np.stack(df['bpe_ids'].values)\\n        self.labels = df['overall'].values - 1  # 0-based classes\\n\\n    def __len__(self):\\n        return len(self.seqs)\\n\\n    def __getitem__(self, idx):\\n        return torch.tensor(self.seqs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\\n\\ndataset = ReviewDataset(df)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 6. MODEL ARCHITECTURE\n",
        "# =========================\n",
        "\n",
        "class EnhancedSentimentModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=50000,\n",
        "        embed_dim=128,\n",
        "        cnn_out=150,\n",
        "        lstm_hidden=126,  # Makes lstm_hidden*2 (252) divisible by num_heads (6)\n",
        "        num_heads=6,\n",
        "        num_classes=5,\n",
        "        dropout=0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Ensure hidden size is divisible by num_heads\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        assert (lstm_hidden * 2) % num_heads == 0, \"lstm_hidden * 2 must be divisible by num_heads\"\n",
        "\n",
        "        # Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # Multi-channel CNN\n",
        "        self.conv3 = nn.Conv1d(embed_dim, cnn_out, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv1d(embed_dim, cnn_out, kernel_size=4, padding=2)\n",
        "        self.conv5 = nn.Conv1d(embed_dim, cnn_out, kernel_size=5, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_out)\n",
        "        self.bn4 = nn.BatchNorm1d(cnn_out)\n",
        "        self.bn5 = nn.BatchNorm1d(cnn_out)\n",
        "\n",
        "        # Residual projection\n",
        "        self.proj = nn.Linear(embed_dim, cnn_out * 3)\n",
        "\n",
        "        # BiLSTM\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=cnn_out * 3,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=2,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.ln_lstm = nn.LayerNorm(lstm_hidden * 2)\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden * 2,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.ln_attn = nn.LayerNorm(lstm_hidden * 2)\n",
        "\n",
        "        # Output\n",
        "        self.fc = nn.Linear(lstm_hidden * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        emb = self.embedding(x)  # (batch, seq, embed_dim)\n",
        "        emb_t = emb.transpose(1, 2)  # (batch, embed_dim, seq)\n",
        "\n",
        "        # Multi-channel CNN\n",
        "        c3 = F.relu(self.bn3(self.conv3(emb_t)))\n",
        "        c4 = F.relu(self.bn4(self.conv4(emb_t)))\n",
        "        c5 = F.relu(self.bn5(self.conv5(emb_t)))\n",
        "\n",
        "        # Adjust the size of c4 to match c3 and c5\n",
        "        c4 = c4[:, :, :-1]  # Remove the extra element from the sequence dimension\n",
        "\n",
        "        # Concatenate CNN outputs\n",
        "        cnn_cat = torch.cat([c3, c4, c5], dim=1)  # (batch, cnn_out*3, seq)\n",
        "        cnn_cat = cnn_cat.transpose(1, 2)  # (batch, seq, cnn_out*3)\n",
        "\n",
        "        # Residual connection\n",
        "        emb_proj = self.proj(emb)  # (batch, seq, cnn_out*3)\n",
        "        x_cnn = cnn_cat + emb_proj\n",
        "\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = self.bilstm(x_cnn)\n",
        "        lstm_out = self.ln_lstm(lstm_out)\n",
        "\n",
        "        # Multi-head attention\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        attn_out = self.ln_attn(lstm_out + attn_out)  # Residual connection\n",
        "\n",
        "        # Global average pooling\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "\n",
        "        # Output\n",
        "        out = self.dropout(pooled)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "HehmQ3y2fKkr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 7. TRAINING & EVALUATION UTILS\n",
        "# =========================\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    patience=4,\n",
        "    max_lr=2e-3,\n",
        "    grad_clip=5.0,\n",
        "    device='cuda'\n",
        "):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=max_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 1cycle scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=max_lr,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for X, y in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Training Loss: {train_loss:.4f}')\n",
        "        print(f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print('Early stopping triggered')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def measure_inference_time(model, loader, device, n_batches=10):\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, (X, _) in enumerate(loader):\n",
        "            if i >= n_batches:\n",
        "                break\n",
        "            X = X.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(X)\n",
        "            times.append(time.time() - start)\n",
        "    avg_time = np.mean(times)\n",
        "    print(f\"Average inference time per batch: {avg_time:.4f} seconds\")\n",
        "    return avg_time\n",
        "\n",
        "def print_memory_footprint():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem = process.memory_info().rss / 1024**2\n",
        "    print(f\"CPU Memory usage: {mem:.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU Allocated:\", torch.cuda.memory_allocated()//1024**2, \"MB\")\n",
        "        print(\"GPU Cached:   \", torch.cuda.memory_reserved()//1024**2, \"MB\")"
      ],
      "metadata": {
        "id": "SckD5HPBfKna"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8. TRAINING EXECUTION\n",
        "# =========================\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = EnhancedSentimentModel(\n",
        "    vocab_size=50000,\n",
        "    embed_dim=128,\n",
        "    cnn_out=150,\n",
        "    lstm_hidden=126,  # Makes lstm_hidden*2 (252) divisible by num_heads (6)\n",
        "    num_heads=6,\n",
        "    num_classes=5,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(\"\\nModel parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Train model\n",
        "trained_model, train_losses, val_losses = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=20,\n",
        "    patience=4,\n",
        "    max_lr=2e-3,\n",
        "    grad_clip=5.0,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfoH9G_FfKq-",
        "outputId": "cec7538e-5e47-40ca-b7dc-f1fd4c506bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnhancedSentimentModel(\n",
            "  (embedding): Embedding(50000, 128, padding_idx=0)\n",
            "  (conv3): Conv1d(128, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv4): Conv1d(128, 150, kernel_size=(4,), stride=(1,), padding=(2,))\n",
            "  (conv5): Conv1d(128, 150, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (bn3): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (proj): Linear(in_features=128, out_features=450, bias=True)\n",
            "  (bilstm): LSTM(450, 126, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (ln_lstm): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=252, out_features=252, bias=True)\n",
            "  )\n",
            "  (ln_attn): LayerNorm((252,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Linear(in_features=252, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n",
            "\n",
            "Model parameters: 7912761\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   1%|          | 12/1265 [01:44<2:56:11,  8.44s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 9. EVALUATION & VISUALIZATION\n",
        "# =========================\n",
        "\n",
        "# Evaluate on test set\n",
        "preds, labels = evaluate_model(trained_model, test_loader, device)\n",
        "print(\"\\nTest Set Classification Report:\")\n",
        "print(classification_report(labels, preds, zero_division=0))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(labels, preds)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[1,2,3,4,5])\n",
        "disp.plot()\n",
        "plt.title(\"Test Set Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(\"Learning Curves\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Measure inference time and memory usage\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "measure_inference_time(trained_model, test_loader, device)\n",
        "print_memory_footprint()"
      ],
      "metadata": {
        "id": "jDS60n3QfVyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyElJM2QfV27"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8Y_TTtifV6g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEGcWL2GfdKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swJqgTohfWLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}